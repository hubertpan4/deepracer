{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepracer Dev Log\n",
    "[_Congrats Richard U., you've managed to nerd snipe me._](https://xkcd.com/356/)\n",
    "\n",
    "Hi this is a dev log for the _redacted_ 2024 DeepRacer competition.\n",
    "\n",
    "DeepRacer is an AWS platform used to introduce developers to Reinforcement Learning.\n",
    "Basically Reinforcement learning is automated supervised learning in the sense that supervised training data is automatically generated from and fed back into the training process.\n",
    "In the case of DeepRacer, we are training a neural network to race a car through an arbitrary track using video image data.\n",
    "This is done by using a \"reward function\" to \"grade\" the successive \"states\" that occur due to the neural network's actions.\n",
    "\n",
    "For example in the case of DeepRacer, we are given a \"params\" dictionary object that contains information about the car's current state.\n",
    "This information includes data about the position, speed, and orientation of the car; along with information about its position relative to the track (is it on or off the track).\n",
    "As the goal of this project is to get the car to race through the track in the least amount of time possible without going off track or crashing;\n",
    "our reward function should reward states that are associated with fast race completion times higher than slower race times or crashes.\n",
    "\n",
    "Information about the properties supplied by the \"params\" dictionary object can be found [here](https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-reward-function-input.html)\n",
    "Sample simple reward functions can be found [here](https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-reward-function-examples.html)\n",
    "\n",
    "Other useful information that I have located is that:\n",
    " - We are using a single camera car (the original deep racer)\n",
    " - The camera pulls images at a rate of 15 frames per second\n",
    " - we can use a restricted set of python libraries (math, random, numpy, scipy, shapely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial thoughts\n",
    "Based off of prior experience with racing go karts, the neural net should probably try to get the car to drive racing lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Plan and Tasks:\n",
    "    - [x] read through documentation\n",
    "\n",
    "## Open questions:\n",
    "    - How do we know if the car has flipped in the simulator? (params.is_crashed?)\n",
    "    - How can we detect skidding in the simulator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization (PPO) Notes\n",
    "\n",
    "PPO was created by OpenAi in 2017 and appears to be the current standard for Reinforcement Learning.\n",
    "Appears to be an incremental improvement on the standard Gradient Descent method but with \"clipping\".\n",
    "Where \"clipping\" refers to the limiting of policy changes between updates to prevent performance collapse.\n",
    "\n",
    "Resources:\n",
    " - https://huggingface.co/learn/deep-rl-course/unit8/introduction \n",
    " - Arxiv PDF: https://arxiv.org/abs/1707.06347\n",
    " - Open ai docs:\n",
    "    - https://spinningup.openai.com/en/latest/algorithms/ppo.html \n",
    "    - https://openai.com/index/openai-baselines-ppo/ \n",
    " - wiki: https://en.wikipedia.org/wiki/Proximal_policy_optimization\n",
    " - Simulator notes:\n",
    "    - https://openai.com/index/roboschool/ (defunct) see https://github.com/Farama-Foundation/Gymnasium and https://farama.org/projects \n",
    "    - joint simulation\n",
    "        - mujoco\n",
    "            - https://gymnasium.farama.org/environments/mujoco/ \n",
    "            - https://github.com/google-deepmind/mujoco \n",
    "            - https://mujoco.readthedocs.io/en/stable/overview.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepracer notes\n",
    "\n",
    "Utilities to process DeepRacer logs: https://github.com/aws-deepracer-community/deepracer-utils \n",
    " - useful for extracting route information\n",
    "Blog on how to train with Racing Lines:\n",
    " - https://mickqg.github.io/DeepracerBlog/ \n",
    " - https://mickqg.github.io/DeepracerBlog/part2.html\n",
    " - https://github.com/MickQG/deepracer-analysis/blob/master/README.md\n",
    "\n",
    "AWS stuff:\n",
    " - https://github.com/aws-solutions-library-samples/guidance-for-training-an-aws-deepracer-model-using-amazon-sagemaker \n",
    " - https://docs.aws.amazon.com/deepracer/latest/student-userguide/reward-function.html\n",
    " - https://github.com/matrousseau/AWS-Deepracer-Optimal-Path-Generator \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Racing Lines:\n",
    "    - https://www.reddit.com/r/coolguides/comments/vw0k49/ideal_racing_lines/ \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapely\n",
    " - https://shapely.readthedocs.io/en/stable/manual.html \n",
    " - https://pypi.org/project/shapely/ \n",
    " - https://github.com/shapely/shapely\n",
    " \n",
    "Spatial analysis library for python, useful for route manipulation.\n",
    "Additional work in [shapelyExp.ipynb](shapelyExp.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
